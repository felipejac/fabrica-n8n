#!/usr/bin/env python3
"""
AI TREND HUNTER - ORQUESTRADOR PRINCIPAL
Sistema automatizado de anÃ¡lise de tendÃªncias em IA e geraÃ§Ã£o de conteÃºdo otimizado

Autor: AI Trend Hunter Bot
Data: 2025-12-12
VersÃ£o: 1.0.0

PIPELINE COMPLETO:
1. DATA LAYER: ExtraÃ§Ã£o de dados do Hugging Face
2. ANALYSIS LAYER: AnÃ¡lise de tendÃªncias e segmentaÃ§Ã£o editorial
3. CONTENT LAYER: GeraÃ§Ã£o de conteÃºdo AEO (Answer Engine Optimization)
4. OUTPUT LAYER: Salvamento com metadados SEO e Schema.org
"""

import sys
import os
import logging
from datetime import datetime
from typing import List, Dict

# Adicionar diretÃ³rio raiz ao path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Importar mÃ³dulos do projeto
from data.fetch_market_data import HuggingFaceDataFetcher
from analysis.trend_analyzer import TrendAnalyzer
from content.aeo_generator import AEOContentGenerator
from output.seo_manager import SEOPostManager
from config import (
    HUGGINGFACE_CONFIG, ANALYSIS_CONFIG, CONTENT_CONFIG, 
    OUTPUT_CONFIG, EXECUTION_CONFIG
)

# Configurar logging
logging.basicConfig(
    level=logging.INFO if EXECUTION_CONFIG['verbose'] else logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('ai_trend_hunter.log')
    ]
)
logger = logging.getLogger(__name__)


class AITrendHunter:
    """Orquestrador principal do sistema AI Trend Hunter"""
    
    def __init__(self):
        """Inicializa todos os componentes do pipeline"""
        logger.info("ğŸš€ Inicializando AI Trend Hunter...")
        
        # Inicializar componentes
        self.data_fetcher = HuggingFaceDataFetcher()
        self.analyzer = TrendAnalyzer()
        self.content_generator = AEOContentGenerator()
        self.seo_manager = SEOPostManager(
            output_dir=OUTPUT_CONFIG['posts_directory']
        )
        
        # Estado
        self.raw_data = None
        self.analyzed_data = None
        self.segments = None
        self.insights = None
        self.generated_posts = []
        
        logger.info("âœ… AI Trend Hunter inicializado com sucesso")
    
    def run(self, topics: List[str] = None) -> Dict[str, any]:
        """
        Executa o pipeline completo de anÃ¡lise e geraÃ§Ã£o de conteÃºdo
        
        Args:
            topics: Lista de tÃ³picos a gerar (None = todos)
            
        Returns:
            DicionÃ¡rio com resultados da execuÃ§Ã£o
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("ğŸ¯ INICIANDO EXECUÃ‡ÃƒO DO AI TREND HUNTER")
        logger.info("=" * 80)
        
        try:
            # ETAPA 1: ExtraÃ§Ã£o de Dados
            logger.info("\nğŸ“¥ ETAPA 1/4: EXTRAÃ‡ÃƒO DE DADOS")
            self._execute_data_extraction()
            
            # ETAPA 2: AnÃ¡lise de TendÃªncias
            logger.info("\nğŸ“Š ETAPA 2/4: ANÃLISE DE TENDÃŠNCIAS")
            self._execute_trend_analysis()
            
            # ETAPA 3: GeraÃ§Ã£o de ConteÃºdo
            logger.info("\nğŸ“ ETAPA 3/4: GERAÃ‡ÃƒO DE CONTEÃšDO")
            self._execute_content_generation(topics)
            
            # ETAPA 4: PÃ³s-processamento
            logger.info("\nğŸ’¾ ETAPA 4/4: PÃ“S-PROCESSAMENTO")
            self._execute_post_processing()
            
            # Calcular duraÃ§Ã£o
            duration = (datetime.now() - start_time).total_seconds()
            
            # Resultado
            result = {
                'success': True,
                'duration_seconds': duration,
                'models_analyzed': len(self.raw_data) if self.raw_data is not None else 0,
                'posts_generated': len(self.generated_posts),
                'post_files': self.generated_posts,
                'execution_date': datetime.now().isoformat()
            }
            
            logger.info("\n" + "=" * 80)
            logger.info("âœ… EXECUÃ‡ÃƒO CONCLUÃDA COM SUCESSO")
            logger.info("=" * 80)
            logger.info(f"â±ï¸  DuraÃ§Ã£o: {duration:.2f} segundos")
            logger.info(f"ğŸ“Š Modelos analisados: {result['models_analyzed']}")
            logger.info(f"ğŸ“ Posts gerados: {result['posts_generated']}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ERRO NA EXECUÃ‡ÃƒO: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'execution_date': datetime.now().isoformat()
            }
    
    def _execute_data_extraction(self):
        """ETAPA 1: Extrai dados do Hugging Face"""
        logger.info("ğŸ” Buscando dados do Hugging Face...")
        
        # Determinar limite baseado no modo
        limit = (
            EXECUTION_CONFIG['test_mode_limit'] 
            if EXECUTION_CONFIG['mode'] == 'test' 
            else HUGGINGFACE_CONFIG['model_limit']
        )
        
        # Buscar dados
        self.raw_data = self.data_fetcher.fetch_market_data(limit=limit)
        
        logger.info(f"âœ… {len(self.raw_data)} modelos extraÃ­dos")
        
        # Salvar dados brutos se configurado
        if EXECUTION_CONFIG['save_raw_data']:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = f"{OUTPUT_CONFIG['data_directory']}/raw_data_{timestamp}.csv"
            self.data_fetcher.save_raw_data(self.raw_data, filepath)
            logger.info(f"ğŸ’¾ Dados brutos salvos: {filepath}")
    
    def _execute_trend_analysis(self):
        """ETAPA 2: Analisa tendÃªncias e segmenta pautas"""
        logger.info("ğŸ“ˆ Analisando tendÃªncias...")
        
        # Analisar dados
        self.analyzed_data = self.analyzer.analyze_trends(self.raw_data)
        
        logger.info(f"âœ… AnÃ¡lise concluÃ­da: {len(self.analyzed_data)} modelos analisados")
        
        # Segmentar pautas editoriais
        logger.info("ğŸ“‹ Segmentando pautas editoriais...")
        self.segments = self.analyzer.segment_editorial_topics(self.analyzed_data)
        
        logger.info(f"âœ… {len(self.segments)} pautas segmentadas:")
        for topic, data in self.segments.items():
            logger.info(f"   - {topic}: {len(data)} modelos")
        
        # Gerar insights
        self.insights = self.analyzer.generate_insights(self.analyzed_data)
        logger.info("âœ… Insights estatÃ­sticos gerados")
    
    def _execute_content_generation(self, topics: List[str] = None):
        """ETAPA 3: Gera conteÃºdo para cada pauta"""
        # Determinar quais tÃ³picos gerar
        if topics:
            topics_to_generate = topics
        elif EXECUTION_CONFIG.get('generation_strategy') == 'daily_rotation':
            # RotaÃ§Ã£o automÃ¡tica baseada no dia da semana
            topics_to_generate = self._get_daily_topics()
        elif EXECUTION_CONFIG.get('topics_to_generate'):
            topics_to_generate = EXECUTION_CONFIG['topics_to_generate']
        else:
            topics_to_generate = CONTENT_CONFIG['editorial_topics']
        
        logger.info(f"ğŸ“ Gerando conteÃºdo para {len(topics_to_generate)} tÃ³picos...")
        
        for topic in topics_to_generate:
            if topic not in self.segments:
                logger.warning(f"âš ï¸  TÃ³pico '{topic}' nÃ£o encontrado, pulando...")
                continue
            
            logger.info(f"   Gerando: {topic}...")
            
            try:
                # Gerar conteÃºdo
                content = self.content_generator.generate_blog_post(
                    topic_type=topic,
                    data_segment=self.segments[topic],
                    insights=self.insights
                )
                
                # Salvar post
                filepath = self.seo_manager.save_post_markdown(
                    content=content,
                    topic_type=topic
                )
                
                self.generated_posts.append(filepath)
                logger.info(f"   âœ… Post salvo: {filepath}")
                
            except Exception as e:
                logger.error(f"   âŒ Erro ao gerar '{topic}': {e}")
        
        logger.info(f"âœ… {len(self.generated_posts)} posts gerados com sucesso")
    
    def _get_daily_topics(self) -> List[str]:
        """
        Retorna os tÃ³picos a gerar baseado no dia da semana
        
        Returns:
            Lista de tÃ³picos para o dia atual
        """
        if not EXECUTION_CONFIG.get('use_weekly_rotation'):
            # Fallback: pegar os N primeiros tÃ³picos
            count = CONTENT_CONFIG.get('daily_posts_count', 2)
            return CONTENT_CONFIG['editorial_topics'][:count]
        
        from datetime import datetime
        
        # Dia da semana (0=segunda, 6=domingo)
        weekday = datetime.now().weekday()
        
        # Buscar tÃ³picos do dia
        daily_topics = CONTENT_CONFIG.get('weekly_rotation', {}).get(weekday, [])
        
        if not daily_topics:
            # Se nÃ£o houver tÃ³picos definidos para hoje (fim de semana)
            logger.info(f"â¸ï¸  Sem artigos programados para hoje ({['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'SÃ¡b', 'Dom'][weekday]})")
            return []
        
        logger.info(f"ğŸ“… Dia da semana: {['Segunda', 'TerÃ§a', 'Quarta', 'Quinta', 'Sexta', 'SÃ¡bado', 'Domingo'][weekday]}")
        logger.info(f"ğŸ“‹ TÃ³picos do dia: {', '.join(daily_topics)}")
        
        return daily_topics
    
    def _execute_post_processing(self):
        """ETAPA 4: PÃ³s-processamento (Ã­ndice, etc)"""
        # Gerar Ã­ndice se configurado
        if EXECUTION_CONFIG['auto_generate_index']:
            logger.info("ğŸ“‘ Gerando Ã­ndice de posts...")
            index_path = self.seo_manager.generate_index()
            logger.info(f"âœ… Ãndice criado: {index_path}")
        
        logger.info("âœ… PÃ³s-processamento concluÃ­do")


def main():
    """FunÃ§Ã£o principal de execuÃ§Ã£o"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          AI TREND HUNTER v1.0.0                           â•‘
â•‘                  Sistema Automatizado de AnÃ¡lise de IA                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Criar e executar orquestrador
    hunter = AITrendHunter()
    result = hunter.run()
    
    # Exibir resumo final
    if result['success']:
        print("\n" + "=" * 80)
        print("ğŸ‰ EXECUÃ‡ÃƒO FINALIZADA COM SUCESSO")
        print("=" * 80)
        print(f"\nğŸ“Š ESTATÃSTICAS:")
        print(f"   â€¢ Modelos analisados: {result['models_analyzed']}")
        print(f"   â€¢ Posts gerados: {result['posts_generated']}")
        print(f"   â€¢ Tempo de execuÃ§Ã£o: {result['duration_seconds']:.2f}s")
        print(f"\nğŸ“ ARQUIVOS GERADOS:")
        for filepath in result['post_files']:
            print(f"   â€¢ {filepath}")
        print("\nâœ… Todos os posts estÃ£o prontos para publicaÃ§Ã£o!")
        print("   Verifique a pasta 'posts/' para os arquivos Markdown")
        print("\n" + "=" * 80)
        
        return 0
    else:
        print("\n" + "=" * 80)
        print("âŒ ERRO NA EXECUÃ‡ÃƒO")
        print("=" * 80)
        print(f"\nâš ï¸  {result.get('error', 'Erro desconhecido')}")
        print("\nğŸ“ Verifique o arquivo 'ai_trend_hunter.log' para mais detalhes")
        print("=" * 80)
        
        return 1


if __name__ == "__main__":
    sys.exit(main())
